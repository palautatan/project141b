nij = c(12, 36, 67, 22)
mij = c(27.68, 20.32, 51.32, 37.68)
ln(nij/mij)
log(nij/mij)
nij*log(nij/mij)
sum(nij*log(nij/mij))
2*sum(nij*log(nij/mij))
log(nij/mij)
log(nij/mij)
2*log(nij/mij)
2*nij*log(nij/mij)
exp(0.1123)
install.packages("vcd")
install.packages('package','/Users/edie/Users/EDIE/Downloads/IMG_3167.JPG/anaconda/lib/R/library)
)
)
q()
install.packages("vcd", "/Users/EDIE/anaconda/lib/R/library")
library(stringr)
library(ggplot2)
library(ggrepel)
install.packages("ggplot2")
install.packages("ggrepel")
library(stringr)
library(ggplot2)
library(ggrepel)
my_coffee = read.csv("/Users/edie/Box Sync/GitThings/https---github.com-palautatan-yumCoffee.git/datasets/coffee_list.csv", header=TRUE)
attach(my_coffee)
Rating = unlist(Personal.Rating)
Rating = gsub("\\/5", "", Rating)
Rating = as.numeric(Rating)
coffee_name = str_split(unlist(Coffee),": ")
coffee_name = sapply(coffee_name, function(coffee_part) {
brand = coffee_part[1]
name = coffee_part[2]
c(brand, name)
}
)
detach(my_coffee)
updated_coffee = cbind(my_coffee, Rating, coffee_name[1,], coffee_name[2,])
names(updated_coffee)[9:10] = c("Brand", "Name")
attach(updated_coffee)
Roast = factor(Roast, levels=c("Special", "Light", "Medium-Light", "Medium", "French", "Medium-Dark", "Dark"))
coffee_plot = ggplot()
coffee_plot = coffee_plot + geom_point(aes(x=Roast, y=Rating)) + geom_label_repel(aes(x=Roast, y=Rating, label=Brand)) + ylim(0,5)
coffee_plot = coffee_plot + ggtitle("Edie's Roasts")
coffee_plot
coffee_plot_2 = ggplot()
coffee_plot_2 = coffee_plot_2 + geom_point(aes(x=Store, y=Rating, colour=factor(Roast)))
coffee_plot_2 = coffee_plot_2 + ggtitle("Edie's Stores")
coffee_plot_2
coffee_plot_2 = ggplot()
coffee_plot_2 = coffee_plot_2 + geom_point(aes(x=Rating, y=Store, colour=factor(Roast)))
coffee_plot_2 = coffee_plot_2 + ggtitle("Edie's Stores")
coffee_plot_2
coffee_plot_2 = ggplot()
coffee_plot_2 = coffee_plot_2 + geom_point(aes(x=Store, y=Rating, colour=factor(Roast)))
coffee_plot_2 = coffee_plot_2 + ggtitle("Edie's Stores")
coffee_plot_2
table_store = rbind(levels(Store), table(Store))
fave_store = table_store[1,which(table_store[2,]==max(table_store[2,]))]
cat("This taster frequently goes to:", fave_store)
avg_score = mean(Rating)
avg_score
df_roast = sapply(levels(Roast), function(this_level) {
these_rows = as.numeric(which(Roast==this_level))
stat = mean(Rating[these_rows])
c(this_level, stat)
}
)
fave_roast = df_roast[1,which(df_roast[2,]==max(df_roast[2,]))]
fave_roast
df_roast
ratings_coffees = rbind(Rating, Name)
ratings_coffees = ratings_coffees[,order(Rating)]
indices = length(ratings_coffees[1,])-c(0:2)
top_three = levels(Name)[ratings_coffees[2, indices]]
top_three
knitr::opts_chunk$set(echo = TRUE)
summary(trials_long)
trials = read.csv("TrialsShort.csv")
trials
48.9402-48.9865
1-pchisq(48.9865-48.9402, 1)
setwd("/Users/EDIE/Box Sync/STA 138/Homework 7")
# WORKING DIRECTORIES
setwd("/Users/EDIE/Box Sync/STA 138/Homework 7")
# LIBRARIES
# PROBLEM 1
internet = read.csv("internet.csv")
head(internet)
names(internet)
# (a) Fit a logistic regression model with Newbie as your response variable,
# and Age, Gender, and Score as your explanatory variables. Write down the
# estimated logistic regression function.
prob1_model = glm(Newbie ~ Age + Gender + score, data=internet, family=binomial(logit))
summary(prob1_model)
# (b) Interpret the value of expB associated with Age in terms of the problem.
exp(-0.06075) # 0.9410585
# (c) Interpret the value of expB associated with Gender in terms of the problem.
exp(1.10870) # 3.030416
# (d) Interpret the value of expB associated with score in terms of the problem.
exp(1.02337) # 2.782556
# PROBLEM 2
# (a) Find and report the 99% profile likelihood confidence intervals for all values of B.
alpha = 0.05/3
confint(prob1_model,level = (1-alpha))
# (b) Using (a), which of your explanatory variables do
# you believe significant effect if someone identifies
# themselves as "new to the Internet"? Explain.
# (c) Predict the probability that a female, aged 28, with a
# doctoral degree identies themselves as "new to the
# Internet".
# (d) Are there any unusual observations in your dataset?
# PROBLEM 3
work = read.csv("work.csv")
head(work)
# (a) Fit and report the estimated logistic regression model
# with coecients for gender, age, and the categories
# for the marriage variable.
prob3_model = glm(obese ~ gender + age + factor(marriage) + min, data=work, family=binomial(logit))
summary(prob3_model)
# (b) How do we use the coecients for marriage to pre-
# dict probabilities at marriage categories? Explain
# what each combination would mean.
# (c) Write down the estimated logistic regression model
# for people who have never been married.
# (d) Write down the estimated logistic regression model
# for people who are married.
4. Continue with problem 3.
(a) Display the Wald Test-statistics and p-values for
testing if each coecient is zero or not.
(b) Based on the above, which variables would you retain
in your model, and why? Assume  = 0:10.
(c) Fit the estimated logistic regression model with only
the variables you chose from (b).
(d) Interpret the coecients of the estimated regression
model you chose in (c).
5. Continue with problem 3 and 4.
(a)Predict the probability that a married women aged
28 who has 400 sedentary minutes per week is obese
using the full model (all rst order predictors, no
interactions).
(b) Predict the probability that a married women aged
28 who has 400 sedentary minutes per week is obese
using the model from 5 (b)-(d).
(c) Using the LR-ratio test, test to see if you can drop the
coecient for gender from the model. Assume the
\full model" is: logit() =  + 1xgender + 2xmin.
Report back the test-statistic and p-value.
(d) Using the LR-ratio test, test to see if you can drop
the coecient for min from the model. Assume the
\full model" is: logit() =  + 1xgender + 2xmin.
Report back the test-statistic and p-value.
6. (a) Write down the estimated logistic regression model
for a female, black student in grade 12.
(b) Write down the estimated logistic regression model
for a male, white student in grade 9.
(c) Predict the probability that the subject has driven a
motor vehicle after consuming substantial amount of
alcohol for a female, black student in grade 12.
(d) Predict the probability that the subject has driven a
motor vehicle after consuming substantial amount of
alcohol for a male, white student in grade 9.
7. Answer the following questions as True or False:
(a) In logistic regression, LR(prole log-likelihood)-
Condence intervals can be found by hand.
(b) In logistic regression, the intercept does not always
have a practical interpretation.
(c) In logistic regression, the larger the absolute value of
i, the more the corresponding X eects (X).
(d) In logistic regression, we can have nominal, ordinal,
and continuous explanatory variables.
8. Use the flu.csv dataset.
(a) Find the best model using forward-backward sub-
set selection using AIC, and report the best tting
model.
(b) Find the best model using forward-backward sub-
set selection using BIC, and report the best tting
model.
(c) Report the AIC and BIC from the model in (b).
(d) Using the model from (b), estimate the probability
that a female aged 54 with an awareness score of 83
would get a
u shot (notice you may not have to use
all of the information given based on your model).
9. Continue with problem 8, and use the model found in
problem 8, part (b).
(a) Use the Hosmer-Lemeshow goodness of t test with
g = 8 to test how well our model is tting. State
the null and alternative hypothesis, the value of the
test-statistic, the p-value, and your conclusion.
(b) Plot a histogram of the standardized residuals. Does
it appear that the assumption that the are standard
normal holds? Why or why not?
(c) Are any values of the standardized residuals larger
than 3? If so, identify what combination of X vari-
ables it was for.
(d) What are the 4 observation/s that most in
u-
enced the change in the coecients (had the largest
DFbeta)? List the observation and the correspond-
ing values of the predictors.
alpha = 0.05/3
confint(prob1_model,level = (1-alpha))
prob1_model
flu = read.csv("flu.csv")
head(flu)
largework = read.csv("largework.csv")
head(largework)
knitr::opts_chunk$set(echo = FALSE)
suppressMessages(confint(prob1_model,level = (1-alpha)))
head(work)
summary(prob3_model)$coefficients
summary(prob3_model)
summary(prob3_model)$coefficients
prob3_model = glm(obese ~ gender + age + factor(marriage), data=work, family=binomial(logit))
summary(prob3_model)$coefficients
summary(prob4_model)$coefficients
prob4_model = glm(obese ~ gender, family = binomial(logit), data = work)
summary(prob4_model)$coefficients
full.model = glm(shot ~ ., data = flu, family = binomial(link=logit))
empty.model = glm(shot ~ 1, data = flu, family = binomial(link=logit))
best.FB.AIC = step(empty.model,scope = list(lower = empty.model, upper = full.model), direction="both")
best.FB.AIC
?step
flu
len(flu)
length(flu)
length(flu[,1])
install.packages(ResourceSelection)
install.packages("ResourceSelection")
install.packages("LogisticDx")
good = dx(best.FB.BIC)
install.packages("pROC")
install.packages("bestglm")
setwd("/Users/EDIE/Box Sync/GitThings/project141b")
predictions = read.csv("predictions.csv")
head(predictions)
head(wordbags)
wordbags = read.csv("blobs.csv")
head(wordbags)
?knn
??knn
install.packages("class")
head(wordbags)
wordbags[,1:2]
names(wordbags)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology)
head(wordbags_simple)
names(wordbags_simple)
dtm <- DocumentTermMatrix(wordbags.blobs)
library(tm)
install.packages("tm")
library(tm)
set.seed(141)
dtm = DocumentTermMatrix(wordbags.blobs)
dtm = DocumentTermMatrix(wordbags$wordbags.blobs)
dtm = DocumentTermMatrix(wordbags[,1])
dtm = DocumentTermMatrix(wordbags_simple[,1])
wordbags = read.csv("blobs.csv", stringsAsFactors = FALSE)
head(wordbags)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology,)
head(wordbags_simple)
names(wordbags_simple)
set.seed(141)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology,)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology)
head(wordbags_simple)
names(wordbags_simple)
set.seed(141)
dtm = DocumentTermMatrix(wordbags_simple[,1])
wordbags_simple[0,1]
wordbags = read.csv("blobs.csv", as.is=TRUE)
head(wordbags)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology)
head(wordbags_simple)
names(wordbags_simple)
set.seed(141)
dtm = DocumentTermMatrix(wordbags_simple[,1])
dtm = DocumentTermMatrix(wordbags_simple[0,1])
wordbags_simple[0,1]
words = Corpus(wordbags_simple[0,1])
words = Corpus(wordbags_simple[,1])
words = Corpus(VectorSource(wordbags_simple[,1]))
dtm = DocumentTermMatrix()
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
dtm = DocumentTermMatrix(words)
mat.df = as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df <= cbind(mat.df, df$Category)
mat.df = cbind(mat.df, df$Category)
colnames(mat.df)[ncol(mat.df)] = "category"
train = sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test = (1:nrow(mat.df))[- train]
cl = mat.df[, "category"]
modeldata = mat.df[,!colnames(mat.df) %in% "category"]
knn.pred = knn(modeldata[train, ], modeldata[test, ], cl[train])
conf.mat = table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy = sum(diag(conf.mat))/length(test) * 100)
library(class)
knn.pred = knn(modeldata[train, ], modeldata[test, ], cl[train])
colnames(mat.df)[ncol(mat.df)] = "category"
train = sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test = (1:nrow(mat.df))[- train]
cl = mat.df[, "category"]
modeldata = mat.df[,!colnames(mat.df) %in% "category"]
knn.pred = knn(modeldata[train, ], modeldata[test, ], cl[train])
conf.mat = table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy = sum(diag(conf.mat))/length(test) * 100)
df.pred <- cbind(knn.pred, modeldata[test, ])
write.table(df.pred, file="output.csv", sep=";")
df.pred
knn.pred
set.seed(141)
words = Corpus(VectorSource(wordbags_simple[,1]))
dtm = DocumentTermMatrix(words)
mat.df = as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df = cbind(mat.df, df$Category)
colnames(mat.df)[ncol(mat.df)] = "category"
train = sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test = (1:nrow(mat.df))[- train]
cl = mat.df[, "category"]
modeldata = mat.df[,!colnames(mat.df) %in% "category"]
knn.pred = knn(modeldata[train, ], modeldata[test, ], cl[train])
conf.mat = table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
df$Category
mat.df
library(class)
library(tm)
wordbags = read.csv("blobs.csv", as.is=TRUE)
head(wordbags)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology)
head(wordbags_simple)
names(wordbags_simple)
set.seed(141)
words = Corpus(VectorSource(wordbags_simple[,1]))
dtm = DocumentTermMatrix(words)
mat.df = as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df = cbind(mat.df, wordbags_simple[,2])
colnames(mat.df)[ncol(mat.df)] = "category"
train = sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test = (1:nrow(mat.df))[- train]
cl = mat.df[, "category"]
modeldata = mat.df[,!colnames(mat.df) %in% "category"]
knn.pred = knn(modeldata[train, ], modeldata[test, ], cl[train])
conf.mat = table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy = sum(diag(conf.mat))/length(test) * 100)
df.pred = cbind(knn.pred, modeldata[test, ])
library(class)
library(tm)
wordbags = read.csv("blobs.csv", as.is=TRUE)
head(wordbags)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology)
head(wordbags_simple)
names(wordbags_simple)
set.seed(141)
words = Corpus(VectorSource(wordbags_simple[,1]))
dtm = DocumentTermMatrix(words)
mat.df = as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df = cbind(mat.df, wordbags_simple[,2])
colnames(mat.df)[ncol(mat.df)] = "category"
train = sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test = (1:nrow(mat.df))[- train]
cl = mat.df[, "category"]
cl
library(class)
library(tm)
wordbags = read.csv("blobs.csv", as.is=TRUE)
head(wordbags)
wordbags_simple = data.frame(wordbags$blobs, wordbags$ideology)
head(wordbags_simple)
names(wordbags_simple)
set.seed(141)
words = Corpus(VectorSource(wordbags_simple[,1]))
dtm = DocumentTermMatrix(words)
dtm
mat.df = as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df
mat.df = cbind(mat.df, wordbags_simple[,2])
head(mat.df)
wordbags_simple[,2]
dim(mat.df)
names(mat.df)
colnames(mat.df)
colnames(mat.df)[ncol(mat.df)] = "category"
train = sample(nrow(mat.df), ceiling(nrow(mat.df) * .50))
test = (1:nrow(mat.df))[- train]
cl = mat.df[, "category"]
cl
train
test
colnames(mat.df)[ncol(mat.df)]
cl = mat.df[, ncol(mat.df)]
cl
modeldata = mat.df[,!colnames(mat.df) %in% "category"]
modeldata
knn.pred = knn(modeldata[train, ], modeldata[test, ], cl[train])
knn.pred
conf.mat = table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy = sum(diag(conf.mat))/length(test) * 100)
df.pred = cbind(knn.pred, modeldata[test, ])
write.table(df.pred, file="output.csv", sep=";")
write.table(df.pred, file="knn_output.csv", sep=";")
df.pred[,1]
head(predictions)
predictions[,1]
predictions[,2]
predictions = read.csv("predictions.csv")
head(predictions)
predictions = read.csv("predictions.csv", as.is=TRUE)
head(predictions)
predictions[,2]
predictions[,3]
predictions[,2]
predictions = read.csv("predictions.csv", stringsAsFactors = FALSE)
head(predictions)
predictions[, "true_ideol"]
compare_predictions = data.frame(knn.pred, predictions[, "pred_ideol"], predictions[, "true_ideol"])
compare_predictions = data.frame(knn.pred, predictions[, "pred_ideol"][test], predictions[, "true_ideol"][test])
